{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "AFTP5DR9jRLU"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries"
      ],
      "metadata": {
        "id": "56Edxrt_FQee"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from keras.layers import Dense, Conv3D, Dropout, Flatten, MaxPooling3D\n",
        "from keras import models, layers, optimizers\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "import zipfile\n",
        "import shutil\n",
        "import tifffile"
      ],
      "metadata": {
        "id": "4C5mWldQCUpI"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unzip File\n",
        "### Make sure to use the respective path"
      ],
      "metadata": {
        "id": "pOYs2jK_Sy8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def unzip_file(zip_file, extract_to):\n",
        "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_to)\n",
        "\n",
        "zip_file = '/content/oneill_20240508_133843.zip'\n",
        "extract_to = '/content'\n",
        "\n",
        "unzip_file(zip_file, extract_to)\n"
      ],
      "metadata": {
        "id": "rX8DLF3PSyXK"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load dataset paths and labels\n",
        "### Make sure to use the respective path"
      ],
      "metadata": {
        "id": "DxnhCZV9Vc8m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = '/content/oneill_20240508_133843'\n",
        "categories = os.listdir(data_dir)\n",
        "file_paths = []\n",
        "labels = []\n",
        "\n",
        "# Store file paths and their respective labels\n",
        "for category in categories:\n",
        "    category_path = os.path.join(data_dir, category)\n",
        "    files = os.listdir(category_path)\n",
        "\n",
        "    for file in files:\n",
        "        file_paths.append(os.path.join(category_path, file))\n",
        "        labels.append(category)"
      ],
      "metadata": {
        "id": "gvX11vqhO0jd"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train-Test Split"
      ],
      "metadata": {
        "id": "pTbjzph7FTxM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train-Test split\n",
        "train_files, test_files, train_labels, test_labels = train_test_split(\n",
        "    file_paths, labels, test_size=0.2, stratify=labels, random_state=42\n",
        ")\n",
        "\n",
        "# Split the training data into training and validation sets\n",
        "train_files, val_files, train_labels, val_labels = train_test_split(\n",
        "    train_files, train_labels, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Make train & test folders to store the dataset\n",
        "train_dir = \"/content/train\"\n",
        "test_dir = \"/content/test\"\n",
        "val_dir = \"/content/val\"\n",
        "\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "os.makedirs(val_dir, exist_ok=True)\n",
        "\n",
        "for category in categories:\n",
        "    os.makedirs(os.path.join(train_dir, category), exist_ok=True)\n",
        "    os.makedirs(os.path.join(test_dir, category), exist_ok=True)\n",
        "    os.makedirs(os.path.join(val_dir, category), exist_ok=True)\n",
        "\n",
        "# Store files in respective train/test directories\n",
        "def move_files(files, labels, destination):\n",
        "    for file, label in zip(files, labels):\n",
        "        dest_folder = os.path.join(destination, label)\n",
        "        try:\n",
        "          shutil.move(file, dest_folder)\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to move {file} to {dest_folder}. Error: {e}\")\n",
        "\n",
        "\n",
        "\n",
        "move_files(train_files, train_labels, train_dir)\n",
        "move_files(test_files, test_labels, test_dir)\n",
        "move_files(val_files, val_labels, val_dir)"
      ],
      "metadata": {
        "id": "pVK0P8IoRwFa"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the train & test folder data\n",
        "def load_tiff_data(data_dir):\n",
        "    images = []\n",
        "    labels = []\n",
        "    class_names = os.listdir(data_dir)\n",
        "    for class_name in class_names:\n",
        "        class_dir = os.path.join(data_dir, class_name)\n",
        "        for file_name in os.listdir(class_dir):\n",
        "            file_path = os.path.join(class_dir, file_name)\n",
        "            with tifffile.TiffFile(file_path) as tif:\n",
        "                image = tif.asarray()  # Load the image data\n",
        "                images.append(image)\n",
        "                labels.append(class_name)\n",
        "\n",
        "    # Convert lists to numpy arrays\n",
        "    images = np.array(images)\n",
        "    labels = np.array(labels)\n",
        "\n",
        "    # Convert class names to integer labels\n",
        "    label_map = {name: i for i, name in enumerate(class_names)}\n",
        "    labels = np.array([label_map[label] for label in labels])\n",
        "\n",
        "    return images, labels\n",
        "\n",
        "train_images, train_labels = load_tiff_data('/content/train')\n",
        "test_images, test_labels = load_tiff_data('/content/test')\n",
        "val_images, val_labels = load_tiff_data('/content/val')"
      ],
      "metadata": {
        "id": "kQh_InHPV6cg"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# One-Hot Encode (Only if classifying using categorical_crossentropy)\n",
        "# DO NOT RUN if the model uses sparse_categorical_crossentropy"
      ],
      "metadata": {
        "id": "AFTP5DR9jRLU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# One-Hot encode labels for classification\n",
        "num_classes = len(np.unique(train_labels))\n",
        "train_labels = tf.keras.utils.to_categorical(train_labels, num_classes)\n",
        "test_labels = tf.keras.utils.to_categorical(test_labels, num_classes)\n",
        "val_labels = tf.keras.utils.to_categorical(val_labels, num_classes)\n"
      ],
      "metadata": {
        "id": "h4oaOgXpXdnF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Balance Dataset"
      ],
      "metadata": {
        "id": "j0jiNaNBT55O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# Compute class weights\n",
        "class_weights = compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(train_labels),\n",
        "    y=train_labels\n",
        ")\n",
        "class_weight_dict = dict(enumerate(class_weights))\n",
        "print(f\"Class weights = {class_weight_dict}\")"
      ],
      "metadata": {
        "id": "iEbZtd3OppMq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f21c5ca-73da-4fbf-c020-6a2c4989ea9b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class weights = {0: 3.2222222222222223, 1: 0.5918367346938775}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the Model"
      ],
      "metadata": {
        "id": "GlQ_TnisUWCr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define model input data shape\n",
        "# The new \"1\" dimension represents the channels\n",
        "input_shape = train_images[0].shape\n",
        "input_shape = (*input_shape, 1)\n",
        "num_labels = len(np.unique(labels))\n",
        "print(f\"Input Shape: {input_shape}  |  Num Labels: {num_labels}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AyrkwPhAFdA7",
        "outputId": "3219ac10-e9c8-4ffb-e149-2d9c09f8c4d8"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Shape: (64, 10, 11, 1)  |  Num Labels: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create CNN model\n",
        "model = Sequential()\n",
        "model.add(Conv3D(32, kernel_size=(3, 3, 3), padding='same', input_shape=input_shape))\n",
        "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "model.add(Conv3D(64, kernel_size=(3, 3, 3), padding='same'))\n",
        "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "model.add(Conv3D(128, kernel_size=(5, 4, 3), padding='same'))\n",
        "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1024, activation=tf.nn.relu))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1024, activation=tf.nn.relu))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1024, activation=tf.nn.relu))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_labels, activation=tf.nn.softmax))\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='adam',\n",
        "            loss='sparse_categorical_crossentropy',\n",
        "            metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(train_images, train_labels,\n",
        "                    epochs=100,\n",
        "                    batch_size=32,\n",
        "                    validation_data=(val_images, val_labels),\n",
        "                    class_weight=class_weight_dict\n",
        "                    )"
      ],
      "metadata": {
        "id": "u4J48yaOF_65",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model in .hdf5 format\n",
        "model.save('/content/test_model.hdf5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Km1_czz-DXvq",
        "outputId": "a89a2d53-5f8b-4f9d-ef67-54c01ea7003e"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Load the model\n",
        "loaded_model = model\n",
        "loaded_model = loaded_model.load_weights('/content/test_model.hdf5')"
      ],
      "metadata": {
        "id": "oiXZ_8hnEAkj"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test the Model"
      ],
      "metadata": {
        "id": "pgKUcaEVcJwr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test set\n",
        "test_loss, test_accuracy = model.evaluate(test_images, test_labels)\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "1c2sYdtncNMj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate predictions for the test set\n",
        "predictions = model.predict(test_images)\n",
        "\n",
        "# Convert predictions to class labels\n",
        "predicted_labels = np.argmax(predictions, axis=1)\n",
        "\n",
        "# If test_labels are one-hot encoded, convert them back to class labels\n",
        "#true_labels = np.argmax(test_labels, axis=1)\n",
        "\n",
        "# Print some of the predictions\n",
        "for i in range(19):\n",
        "    print(f\"True label: {test_labels[i]}, Predicted label: {predicted_labels[i]}\")\n"
      ],
      "metadata": {
        "id": "NNkml8MXcm9i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "class_names = np.unique(train_labels)\n",
        "# Create confusion matrix\n",
        "cm = confusion_matrix(test_labels, predicted_labels)\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "eanYYkyMc3PK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Extract loss values from history\n",
        "history_dict = history.history\n",
        "loss_values = history_dict['loss']\n",
        "val_loss_values = history_dict.get('val_loss', [])  # Handle cases where validation loss might not be present\n",
        "\n",
        "# Create a plot for loss\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(loss_values, 'o-', label='Training Loss')\n",
        "plt.plot(val_loss_values, 'o-', label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "V0PMUdEhePKu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract accuracy values from history\n",
        "accuracy_values = history_dict['accuracy']\n",
        "val_accuracy_values = history_dict.get('val_accuracy', [])  # Handle cases where validation accuracy might not be present\n",
        "\n",
        "# Create a plot for accuracy\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(accuracy_values, 'o-', label='Training Accuracy')\n",
        "plt.plot(val_accuracy_values, 'o-', label='Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xyeHMa1feycc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Zip Dataset"
      ],
      "metadata": {
        "id": "n5va5PUPBRBy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Paths to the folders you want to zip\n",
        "folder1 = \"/content/train\"\n",
        "folder2 = \"/content/test\"\n",
        "folder3 = \"/content/val\"\n",
        "\n",
        "# Define the name of the output zip file\n",
        "output_zip = \"/content/dataset.zip\"\n",
        "\n",
        "# Create a zip file containing the three folders\n",
        "with zipfile.ZipFile(output_zip, 'w') as zipf:\n",
        "    for folder in [folder1, folder2, folder3]:\n",
        "        # Walk through each folder and add files to the zip\n",
        "        for root, dirs, files in os.walk(folder):\n",
        "            for file in files:\n",
        "                # Create the full path to the file\n",
        "                full_path = os.path.join(root, file)\n",
        "                # Add file to the zip file with an appropriate archive name\n",
        "                archive_name = os.path.relpath(full_path, os.path.dirname(folder1))\n",
        "                zipf.write(full_path, arcname=archive_name)\n",
        "\n",
        "print(f\"Zipped folders into {output_zip}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78De64kOBUnp",
        "outputId": "05a38237-d4d5-4a5f-912f-c07dbe0df370"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Zipped folders into /content/dataset.zip\n"
          ]
        }
      ]
    }
  ]
}